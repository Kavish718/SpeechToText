{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-10T01:15:07.921432Z",
     "start_time": "2024-11-10T01:15:01.919823Z"
    }
   },
   "source": [
    "!pip install python_speech_features\n",
    "!pip install pydub librosa scipy\n",
    "!pip install librosa scipy tqdm\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python_speech_features in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (0.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydub in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (0.25.1)\n",
      "Requirement already satisfied: librosa in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (1.14.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from lazy-loader>=0.1->librosa) (24.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (0.10.2.post1)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (1.14.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from lazy-loader>=0.1->librosa) (24.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.6)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\documents\\nus\\cs5242\\final project\\eda_libraspeech\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T16:36:26.722465Z",
     "start_time": "2024-11-09T16:36:25.556188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from python_speech_features import mfcc, delta"
   ],
   "id": "7389b1229cbc1e74",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-09T17:30:08.877996Z",
     "start_time": "2024-11-09T17:05:42.016805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# !pip install librosa numpy sklearn tqdm\n",
    "\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "input_dir = r'C:\\Users\\User\\Documents\\NUS\\CS5242\\Final Project\\EDA_libraspeech\\LibriSpeech\\train-clean-100'  # Use raw strings\n",
    "output_dir = r'C:\\Users\\User\\Documents\\NUS\\CS5242\\Final Project\\EDA_libraspeech\\processed_data' \n",
    "transcriptions = {}\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "def load_transcriptions(input_dir):\n",
    "    for root, _, _ in os.walk(input_dir):\n",
    "        for trans_file in glob.glob(os.path.join(root, \"*.trans.txt\")):\n",
    "            with open(trans_file, \"r\") as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split(\" \", 1)\n",
    "                    if len(parts) == 2:\n",
    "                        file_id, transcription = parts\n",
    "                        transcriptions[file_id] = transcription.lower() \n",
    "\n",
    "load_transcriptions(input_dir)\n",
    "audio_files = []\n",
    "file_ids = []\n",
    "for root, _, files in os.walk(input_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.flac'):\n",
    "            file_path = os.path.join(root, file)\n",
    "            file_id = os.path.splitext(file)[0] \n",
    "            if file_id in transcriptions:\n",
    "                audio_files.append(file_path)\n",
    "                file_ids.append(file_id)\n",
    "\n",
    "train_files, test_files, train_file_ids, test_file_ids = train_test_split(\n",
    "    audio_files, file_ids, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Total files: {len(audio_files)}, Training: {len(train_files)}, Testing: {len(test_files)}\")\n",
    "\n",
    "def process_file(file_path, file_id):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=16000)\n",
    "        y, _ = librosa.effects.trim(y)\n",
    "        y = y - np.mean(y)\n",
    "\n",
    "        mfcc_features = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        delta_features = librosa.feature.delta(mfcc_features, order=1)\n",
    "        delta2_features = librosa.feature.delta(mfcc_features, order=2)\n",
    "        features = np.vstack((mfcc_features, delta_features, delta2_features)).T\n",
    "\n",
    "        return features, file_id\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None, file_id\n",
    "\n",
    "batch_size = 1000  \n",
    "\n",
    "scaler = StandardScaler()\n",
    "ipca = IncrementalPCA(n_components=20)\n",
    "\n",
    "def training_data_generator(files, file_ids, batch_size):\n",
    "    total_files = len(files)\n",
    "    for i in range(0, total_files, batch_size):\n",
    "        batch_files = files[i:i+batch_size]\n",
    "        batch_file_ids = file_ids[i:i+batch_size]\n",
    "        batch_features = []\n",
    "        batch_lengths = []\n",
    "        for file_path, file_id in zip(batch_files, batch_file_ids):\n",
    "            features, fid = process_file(file_path, file_id)\n",
    "            if features is not None:\n",
    "                batch_features.append(features)\n",
    "                batch_lengths.append(len(features))\n",
    "            else:\n",
    "                print(f\"Failed to process {file_id}\")\n",
    "        if batch_features:\n",
    "            batch_features_concat = np.concatenate(batch_features, axis=0)\n",
    "            yield batch_features_concat\n",
    "\n",
    "print(\"Processing and fitting StandardScaler and IncrementalPCA on training data...\")\n",
    "\n",
    "for batch_features in tqdm(training_data_generator(train_files, train_file_ids, batch_size), total=int(len(train_files)/batch_size)+1):\n",
    "    scaler.partial_fit(batch_features)\n",
    "\n",
    "for batch_features in tqdm(training_data_generator(train_files, train_file_ids, batch_size), total=int(len(train_files)/batch_size)+1):\n",
    "    batch_features_scaled = scaler.transform(batch_features)\n",
    "    ipca.partial_fit(batch_features_scaled)\n",
    "\n",
    "with open(os.path.join(output_dir, 'scaler.pkl'), 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "with open(os.path.join(output_dir, 'ipca.pkl'), 'wb') as f:\n",
    "    pickle.dump(ipca, f)\n",
    "print(\"Scaler and IncrementalPCA models saved.\")\n",
    "\n",
    "def process_transform_save(file_path, file_id):\n",
    "    features, fid = process_file(file_path, file_id)\n",
    "    if features is not None:\n",
    "        features_scaled = scaler.transform(features)\n",
    "        features_pca = ipca.transform(features_scaled)\n",
    "        output_file = os.path.join(output_dir, f\"{file_id}_features.npy\")\n",
    "        np.save(output_file, features_pca)\n",
    "    else:\n",
    "        print(f\"Failed to process {file_id}\")\n",
    "\n",
    "print(\"Processing, transforming, and saving training features...\")\n",
    "for file_path, file_id in tqdm(zip(train_files, train_file_ids), total=len(train_files)):\n",
    "    process_transform_save(file_path, file_id)\n",
    "\n",
    "print(\"Processing, transforming, and saving testing features...\")\n",
    "for file_path, file_id in tqdm(zip(test_files, test_file_ids), total=len(test_files)):\n",
    "    process_transform_save(file_path, file_id)\n",
    "\n",
    "train_transcriptions = {file_id: transcriptions[file_id] for file_id in train_file_ids}\n",
    "test_transcriptions = {file_id: transcriptions[file_id] for file_id in test_file_ids}\n",
    "\n",
    "with open(os.path.join(output_dir, \"train_transcriptions.json\"), \"w\") as f:\n",
    "    json.dump(train_transcriptions, f)\n",
    "\n",
    "with open(os.path.join(output_dir, \"test_transcriptions.json\"), \"w\") as f:\n",
    "    json.dump(test_transcriptions, f)\n",
    "\n",
    "print(\"Transcriptions saved.\")\n",
    "print(\"Preprocessing complete.\")\n"
   ],
   "id": "b8c14c05be7045a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 28539, Training: 22831, Testing: 5708\n",
      "Processing and fitting StandardScaler and IncrementalPCA on training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [06:43<00:00, 17.54s/it]\n",
      "100%|██████████| 23/23 [07:00<00:00, 18.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler and IncrementalPCA models saved.\n",
      "Processing, transforming, and saving training features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22831/22831 [08:33<00:00, 44.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing, transforming, and saving testing features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5708/5708 [02:07<00:00, 44.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcriptions saved.\n",
      "Preprocessing complete.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T08:56:37.321273Z",
     "start_time": "2024-11-10T08:13:51.086938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from hmmlearn import hmm\n",
    "import pickle\n",
    "import multiprocessing\n",
    "import nltk\n",
    "\n",
    "nltk.download('cmudict')\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "output_dir = r'C:\\Users\\User\\Documents\\NUS\\CS5242\\Final Project\\EDA_libraspeech\\processed_data'\n",
    "\n",
    "# Global variables\n",
    "X_train = []\n",
    "lengths_train = []\n",
    "y_train = []\n",
    "\n",
    "with open(os.path.join(output_dir, \"train_transcriptions.json\"), \"r\") as f:\n",
    "    train_transcriptions = json.load(f)\n",
    "\n",
    "pronunciation_dict = cmudict.dict()\n",
    "\n",
    "# todo: to refine here?\n",
    "phoneme_groups = {\n",
    "    'vowels': ['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'EH', 'ER', 'EY', 'IH', 'IY', 'OW', 'OY', 'UH', 'UW'],\n",
    "    'stops': ['P', 'B', 'T', 'D', 'K', 'G'],\n",
    "    'fricatives': ['F', 'V', 'TH', 'DH', 'S', 'Z', 'SH', 'ZH', 'HH'],\n",
    "    'affricates': ['CH', 'JH'],\n",
    "    'nasals': ['M', 'N', 'NG'],\n",
    "    'liquids_glides': ['L', 'R', 'W', 'Y'],\n",
    "    'silence': ['SIL']\n",
    "}\n",
    "\n",
    "phoneme_to_group = {}\n",
    "for group, phonemes in phoneme_groups.items():\n",
    "    for phoneme in phonemes:\n",
    "        phoneme_to_group[phoneme] = group\n",
    "\n",
    "def words_to_phoneme_groups(words, pronunciation_dict, phoneme_to_group):\n",
    "    phoneme_groups_sequence = []\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        if word in pronunciation_dict:\n",
    "            phonemes = pronunciation_dict[word][0]\n",
    "            for phoneme in phonemes:\n",
    "                phoneme = ''.join([char for char in phoneme if char.isalpha()])  # Remove stress markers\n",
    "                phoneme = phoneme.upper()\n",
    "                group = phoneme_to_group.get(phoneme)\n",
    "                if group:\n",
    "                    phoneme_groups_sequence.append(group)\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            pass\n",
    "    return phoneme_groups_sequence\n",
    "\n",
    "train_phoneme_sequences = {}\n",
    "for file_id, transcription in train_transcriptions.items():\n",
    "    words = transcription.strip().split()\n",
    "    phoneme_groups_sequence = words_to_phoneme_groups(words, pronunciation_dict, phoneme_to_group)\n",
    "    train_phoneme_sequences[file_id] = phoneme_groups_sequence\n",
    "\n",
    "#mapping from phoneme groups to labels\n",
    "unique_phoneme_groups = set()\n",
    "for groups in train_phoneme_sequences.values():\n",
    "    unique_phoneme_groups.update(groups)\n",
    "\n",
    "phoneme_group_to_label = {group: idx for idx, group in enumerate(sorted(unique_phoneme_groups))}\n",
    "label_to_phoneme_group = {idx: group for group, idx in phoneme_group_to_label.items()}\n",
    "\n",
    "with open(os.path.join(output_dir, \"phoneme_group_to_label.json\"), \"w\") as f:\n",
    "    json.dump(phoneme_group_to_label, f)\n",
    "\n",
    "def align_phonemes_to_frames(num_frames, phonemes):\n",
    "    frames_per_phoneme = max(int(num_frames / len(phonemes)), 1)\n",
    "    labels = []\n",
    "    for phoneme in phonemes:\n",
    "        labels.extend([phoneme] * frames_per_phoneme)\n",
    "    if len(labels) < num_frames:\n",
    "        labels.extend([phonemes[-1]] * (num_frames - len(labels)))\n",
    "    else:\n",
    "        labels = labels[:num_frames]\n",
    "    return labels\n",
    "\n",
    "for file_id in train_transcriptions.keys():\n",
    "    features = np.load(os.path.join(output_dir, f\"{file_id}_features.npy\"))\n",
    "    num_frames = features.shape[0]\n",
    "    \n",
    "    phoneme_groups = train_phoneme_sequences[file_id]\n",
    "    \n",
    "    labels = align_phonemes_to_frames(num_frames, phoneme_groups)\n",
    "    labels = [phoneme_group_to_label[group] for group in labels]\n",
    "    \n",
    "    X_train.append(features)\n",
    "    lengths_train.append(num_frames)\n",
    "    y_train.extend(labels)\n",
    "\n",
    "def train_phoneme_group_model(args):\n",
    "    group, label = args\n",
    "    try:\n",
    "        group_features = []\n",
    "        for i, features in enumerate(X_train):\n",
    "            start = sum(lengths_train[:i])\n",
    "            end = start + lengths_train[i]\n",
    "            labels = y_train[start:end]\n",
    "            \n",
    "            group_indices = [idx for idx, l in enumerate(labels) if l == label]\n",
    "            if group_indices:\n",
    "                group_features.extend(features[group_indices])\n",
    "        \n",
    "        group_features = np.array(group_features)\n",
    "        \n",
    "        if len(group_features) > 0:\n",
    "            model = hmm.GaussianHMM(n_components=10, covariance_type='diag', n_iter=100)\n",
    "            model.fit(group_features)\n",
    "            with open(os.path.join(output_dir, f\"{group}_model.pkl\"), \"wb\") as f:\n",
    "                pickle.dump(model, f)\n",
    "            print(f\"Trained model for group {group}\")\n",
    "        else:\n",
    "            print(f\"No data for group {group}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error training model for group {group}: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    group_args = [(group, label) for group, label in phoneme_group_to_label.items()]\n",
    "    \n",
    "    #train models sequentially since parallel is not working apparently\n",
    "    for args in group_args:\n",
    "        train_phoneme_group_model(args)\n",
    "\n"
   ],
   "id": "4fb291c86f0d7b59",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "Model is not converging.  Current: -2835955.849360123 is not greater than -2835955.5610560817. Delta is -0.2883040411397815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model for group affricates\n",
      "Trained model for group fricatives\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not converging.  Current: -28676227.404248703 is not greater than -28675989.35240077. Delta is -238.0518479347229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model for group liquids_glides\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not converging.  Current: -32491031.962262094 is not greater than -32490690.52566058. Delta is -341.43660151585937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model for group nasals\n",
      "Trained model for group stops\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not converging.  Current: -96825341.2409979 is not greater than -96822600.59224094. Delta is -2740.6487569510937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model for group vowels\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:30:42.444612Z",
     "start_time": "2024-11-12T11:30:42.431274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def viterbi_decode(features, emission_models, transition_matrix, start_prob):\n",
    "    T = features.shape[0]\n",
    "    N = len(emission_models)\n",
    "    log_prob = np.zeros((T, N))\n",
    "    backpointer = np.zeros((T, N), dtype=int)\n",
    "\n",
    "    for s in range(N):\n",
    "        group = label_to_phoneme_group[s]\n",
    "        emission_log_prob = emission_models[group].score(features[0].reshape(1, -1))\n",
    "        log_prob[0, s] = np.log(start_prob[s]) + emission_log_prob\n",
    "\n",
    "    for t in range(1, T):\n",
    "        for s in range(N):\n",
    "            group = label_to_phoneme_group[s]\n",
    "            emission_log_prob = emission_models[group].score(features[t].reshape(1, -1))\n",
    "            transition_log_probs = log_prob[t - 1] + np.log(transition_matrix[:, s])\n",
    "            best_prev_state = np.argmax(transition_log_probs)\n",
    "            log_prob[t, s] = transition_log_probs[best_prev_state] + emission_log_prob\n",
    "            backpointer[t, s] = best_prev_state\n",
    "\n",
    "    best_path = np.zeros(T, dtype=int)\n",
    "    best_path[T - 1] = np.argmax(log_prob[T - 1])\n",
    "    for t in range(T - 2, -1, -1):\n",
    "        best_path[t] = backpointer[t + 1, best_path[t + 1]]\n",
    "\n",
    "    return best_path\n"
   ],
   "id": "232534effdacf9f1",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:30:57.659807Z",
     "start_time": "2024-11-12T11:30:57.275211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "for groups in train_phoneme_sequences.values():\n",
    "    for i in range(len(groups) - 1):\n",
    "        bigram_counts[groups[i]][groups[i + 1]] += 1\n",
    "\n",
    "phoneme_groups = sorted(phoneme_group_to_label.keys())\n",
    "n_states = len(phoneme_groups)\n",
    "transition_matrix = np.zeros((n_states, n_states))\n",
    "\n",
    "for i, group_i in enumerate(phoneme_groups):\n",
    "    total = sum(bigram_counts[group_i].values())\n",
    "    if total > 0:\n",
    "        for j, group_j in enumerate(phoneme_groups):\n",
    "            count = bigram_counts[group_i][group_j]\n",
    "            transition_matrix[i, j] = count / total\n",
    "    else:\n",
    "        transition_matrix[i, :] = 1 / n_states\n"
   ],
   "id": "c990e81e1d7e90eb",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T11:54:24.351112Z",
     "start_time": "2024-11-12T11:31:01.359196Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import editdistance\n",
    "\n",
    "with open(os.path.join(output_dir, \"test_transcriptions.json\"), \"r\") as f:\n",
    "    test_transcriptions = json.load(f)\n",
    "\n",
    "with open(os.path.join(output_dir, \"phoneme_group_to_label.json\"), \"r\") as f:\n",
    "    phoneme_group_to_label = json.load(f)\n",
    "label_to_phoneme_group = {int(idx): group for group, idx in phoneme_group_to_label.items()}\n",
    "\n",
    "X_test = []\n",
    "lengths_test = []\n",
    "test_phoneme_sequences = {}\n",
    "\n",
    "for file_id, transcription in test_transcriptions.items():\n",
    "    features = np.load(os.path.join(output_dir, f\"{file_id}_features.npy\"))\n",
    "    X_test.append(features)\n",
    "    lengths_test.append(features.shape[0])\n",
    "\n",
    "    words = transcription.strip().split()\n",
    "    phoneme_groups_sequence = words_to_phoneme_groups(words, pronunciation_dict, phoneme_to_group)\n",
    "    test_phoneme_sequences[file_id] = phoneme_groups_sequence\n",
    "\n",
    "emission_models = {}\n",
    "phoneme_groups = sorted(phoneme_group_to_label.keys())\n",
    "n_states = len(phoneme_groups)\n",
    "\n",
    "for group in phoneme_groups:\n",
    "    model_path = os.path.join(output_dir, f\"{group}_model.pkl\")\n",
    "    with open(model_path, \"rb\") as f:\n",
    "        emission_models[group] = pickle.load(f)\n",
    "\n",
    "predicted_sequences = {}\n",
    "start_prob = np.full(n_states, 1 / n_states)\n",
    "\n",
    "for idx, features in enumerate(X_test):\n",
    "    best_path = viterbi_decode(features, emission_models, transition_matrix, start_prob)\n",
    "    predicted_phoneme_groups = [label_to_phoneme_group[state] for state in best_path]\n",
    "\n",
    "    collapsed_sequence = [predicted_phoneme_groups[0]]\n",
    "    for group in predicted_phoneme_groups[1:]:\n",
    "        if group != collapsed_sequence[-1]:\n",
    "            collapsed_sequence.append(group)\n",
    "\n",
    "    file_id = list(test_transcriptions.keys())[idx]\n",
    "    predicted_sequences[file_id] = collapsed_sequence\n",
    "\n",
    "total_errors = 0\n",
    "total_length = 0\n",
    "\n",
    "for file_id in test_transcriptions.keys():\n",
    "    predicted = predicted_sequences[file_id]\n",
    "    ground_truth = test_phoneme_sequences[file_id]\n",
    "\n",
    "    distance = editdistance.eval(predicted, ground_truth)\n",
    "    total_errors += distance\n",
    "    total_length += len(ground_truth)\n",
    "\n",
    "phoneme_group_error_rate = total_errors / total_length\n",
    "print(f\"Phoneme Group Error Rate: {phoneme_group_error_rate * 100:.2f}%\")\n"
   ],
   "id": "e895a38b42641468",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phoneme Group Error Rate: 179.66%\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
